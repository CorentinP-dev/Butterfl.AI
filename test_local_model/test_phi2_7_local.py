from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_name = "microsoft/phi-2"

print("‚è≥ T√©l√©chargement et chargement du mod√®le Phi-2 en float32 sur CPU...")

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,
    device_map="cpu"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

system_prompt = """
Tu es un narrateur d'histoires alternatives pour un jeu narratif interactif appel√© 'And if ...'.
Ton r√¥le :
- Cr√©er des r√©cits immersifs et coh√©rents bas√©s sur des changements historiques propos√©s par le joueur.
- D√©crire les cons√©quences et l'√©volution du monde avec richesse et d√©tails.
- Proposer 3 choix d'actions interactives √† la fin de chaque g√©n√©ration, permettant au joueur d'influencer le cours de l'histoire.
- Maintenir une continuit√© logique et √©viter les contradictions dans le r√©cit.
Tu es cr√©atif, pr√©cis, et immersif.
"""

print("‚úÖ Mod√®le charg√© avec succ√®s sur CPU en float32 !")

while True:
    user_input = input("Proposez un changement historique pour commencer l'histoire (ou 'exit' pour quitter) : ")
    if user_input.lower() in ["exit", "quit", "stop"]:
        print("Fin du test.")
        break

    prompt = f"""
    {system_prompt}

    üìú **Contexte initial** :
    {user_input}
    """

    inputs = tokenizer(prompt, return_tensors="pt").to("cpu")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=500,
            do_sample=True,
            top_k=40,
            top_p=0.92,
            temperature=0.7,
            repetition_penalty=1.2
        )

    print("\nüìù Histoire g√©n√©r√©e :")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
    print("\n---")
